[
  {
    "id" : "3c4e3720-e10f-4820-9d49-0bbe2872fe66",
    "prId" : 5315,
    "comments" : [
      {
        "id" : "9986d53f-f649-4101-982f-c1de3c53b0d9",
        "parentId" : null,
        "author" : {
          "login" : "miklos-da",
          "name" : "Miklos",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/57664299?v=4"
        },
        "body" : "Correct the wrapping of this line as well, please.",
        "createdAt" : "2020-04-01T15:22:19Z",
        "updatedAt" : "2020-04-01T15:23:05Z",
        "lastEditedBy" : {
          "login" : "miklos-da",
          "name" : "Miklos",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/57664299?v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "2beee020cac3ccd86a87358e05512687af93e7d4",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +435,439 @@  ): Future[LedgerId] = {\n    // Note that here we only store the ledger entry and we do not update anything else, such as the\n    // headRef. This is OK since this initialization\n    // step happens before we start up the sql ledger at all, so it's running in isolation.\n"
  },
  {
    "id" : "10418641-68ad-43e8-82cb-83720b05589c",
    "prId" : 5100,
    "comments" : [
      {
        "id" : "86b5ba37-8100-4976-89e5-888dea3867a4",
        "parentId" : null,
        "author" : {
          "login" : "miklos-da",
          "name" : "Miklos",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/57664299?v=4"
        },
        "body" : "cannot",
        "createdAt" : "2020-03-25T11:26:40Z",
        "updatedAt" : "2020-03-25T11:28:29Z",
        "lastEditedBy" : {
          "login" : "miklos-da",
          "name" : "Miklos",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/57664299?v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "d421ae1a437d89063df1c3e40078f52885762eaf",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +184,188 @@      .get()\n      .fold[Either[String, Unit]](\n        Left(\"No ledger configuration available, can not validate ledger time\")\n      )(\n        config => config.timeModel.checkTime(ledgerTime, recordTime)"
  },
  {
    "id" : "ed4af607-6d25-4326-aa6c-b2255ed519a3",
    "prId" : 2131,
    "comments" : [
      {
        "id" : "57029f9c-5d93-45e6-94ec-d376d0282619",
        "parentId" : null,
        "author" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "body" : "This makes the code less resistant against adding a new case to `PersistenceResponse` (e.g., `PersistenceResponse.Error`). I don't have a strong opinion, but my personal preference would be listing both cases explicitly.",
        "createdAt" : "2019-07-16T14:09:30Z",
        "updatedAt" : "2019-07-29T11:11:32Z",
        "lastEditedBy" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "6ca6d77a-f6e6-4acb-b026-0322311f5add",
        "parentId" : "57029f9c-5d93-45e6-94ec-d376d0282619",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "It's not clear to me why we should change the api of `LedgerDao` to return a `Map`, but then immediately discard the value with `_`. ",
        "createdAt" : "2019-07-17T07:21:55Z",
        "updatedAt" : "2019-07-29T11:11:32Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "6a58f1a9-9542-4637-b7ad-5ecb6f762bca",
        "parentId" : "57029f9c-5d93-45e6-94ec-d376d0282619",
        "author" : {
          "login" : "stefanobaghino-da",
          "name" : "Stefano Baghino",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/43749967?v=4"
        },
        "body" : "It's not strictly necessary right now, but since it's information that it's available to the data access object I see no reason to discard it upstream either. Furthermore, it would be very difficult to understand in which case to return `Ok` and in which return `Duplicate` if the end result is mixed. Do you think we should add the concept of `Duplicate` to this layer as well?",
        "createdAt" : "2019-07-26T15:06:19Z",
        "updatedAt" : "2019-07-29T11:11:32Z",
        "lastEditedBy" : {
          "login" : "stefanobaghino-da",
          "name" : "Stefano Baghino",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/43749967?v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "32b9c53286fc57cb866c7a586b41bb9fdf2064f6",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +360,364 @@        // discard the information; package upload is idempotent, apart from the fact\n        // that we only keep the knownSince and sourceDescription of the first upload.\n        UploadPackagesResult.Ok\n      }(DEC)\n  }"
  },
  {
    "id" : "26184f1c-22d8-4e05-98ea-fc3c167ec087",
    "prId" : 1818,
    "comments" : [
      {
        "id" : "15ab13ab-dcfe-48e3-99c5-0f2ec7f59b68",
        "parentId" : null,
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "Not sure I understand. Why do we use `InMemoryPackageStore` for the SqlLedger?",
        "createdAt" : "2019-06-24T07:52:21Z",
        "updatedAt" : "2019-07-02T12:24:58Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "6fb9ffd2-fe7e-46a8-89e5-d9a216ec5b58",
        "parentId" : "15ab13ab-dcfe-48e3-99c5-0f2ec7f59b68",
        "author" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "body" : "This PR is just WIP so far. First part was to move ownership of the package store to the `Ledger` instance. Next part will be copying the given `InMemoryPackageStore` (populated by the CLI arguments and scenario loader) to the PostgreSQL database on startup, and then serving package management calls from the database.",
        "createdAt" : "2019-06-24T14:27:31Z",
        "updatedAt" : "2019-07-02T12:24:58Z",
        "lastEditedBy" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "7a532f9a-41d1-437c-9f57-192c6e446b97",
        "parentId" : "15ab13ab-dcfe-48e3-99c5-0f2ec7f59b68",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "Alright, I'll shut up now and wait for your signal :)",
        "createdAt" : "2019-06-24T14:28:14Z",
        "updatedAt" : "2019-07-02T12:24:58Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "bae23a0a96200d0ece436a646cceb68a50511f37",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +77,81 @@      timeProvider: TimeProvider,\n      acs: InMemoryActiveContracts,\n      packages: InMemoryPackageStore,\n      initialLedgerEntries: ImmArray[LedgerEntryWithLedgerEndIncrement],\n      queueDepth: Int,"
  },
  {
    "id" : "f1d3aa99-a66d-4a8f-9922-0e8db761e49e",
    "prId" : 1818,
    "comments" : [
      {
        "id" : "43a34c68-a21b-4ebd-a8f1-1214b5726e6b",
        "parentId" : null,
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "I'd rather we don't thread the `InMemoryPackageStore` all the way to the Ledger initialization, but rather we simply initialize the ledger and automatically upload the packages from \"the outside\" via the regular mechanism. What do you think? Duplicate packages will be \"ignored\" and the other packages just uploaded.",
        "createdAt" : "2019-06-28T08:33:09Z",
        "updatedAt" : "2019-07-02T12:24:58Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "80882400-c64e-42ab-9156-a3d38ce0683d",
        "parentId" : "43a34c68-a21b-4ebd-a8f1-1214b5726e6b",
        "author" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "body" : "Not sure about that. This `initialize` method also inserts the provided ACS and ledger entries (generated by the scenario loader) into the database, and conceptually the packages should be added before the ledger entries.\r\n\r\nIn practice, nothing would happen if we first copied the given ledger entries and ACS upon initialization, and then uploaded the packages later from the outside.",
        "createdAt" : "2019-06-28T09:08:50Z",
        "updatedAt" : "2019-07-02T12:24:58Z",
        "lastEditedBy" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "cd6e653f-068d-445f-a946-897cfde85889",
        "parentId" : "43a34c68-a21b-4ebd-a8f1-1214b5726e6b",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "I see. Looking at my postgres-index(er) branch, I see that it won't be a problem. 👍 ",
        "createdAt" : "2019-06-28T09:17:07Z",
        "updatedAt" : "2019-07-02T12:24:58Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "bae23a0a96200d0ece436a646cceb68a50511f37",
    "line" : 178,
    "diffHunk" : "@@ -1,1 +436,440 @@                  s\"Initial ledger entries provided, presumably from scenario, but I'm picking up from an existing database, and thus they will not be used\")\n              }\n              if (packages.listLfPackagesSync().nonEmpty) {\n                logger.warn(\n                  s\"Initial packages provided, presumably as command line arguments, but I'm picking up from an existing database, and thus they will not be used\")"
  },
  {
    "id" : "152744e4-62b4-4672-84e1-527e99967cd4",
    "prId" : 1505,
    "comments" : [
      {
        "id" : "ef90bb11-8272-4300-9c6b-f991bbcc5c7b",
        "parentId" : null,
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "That's merely ignoring the error, right? Which means that we actually lose the ledger entry. We probably should rather retry n times and then crash the application?",
        "createdAt" : "2019-06-04T07:58:15Z",
        "updatedAt" : "2019-06-04T09:19:19Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "e1efd4ee-17d2-4958-8b2f-085e0d3dac45",
        "parentId" : "ef90bb11-8272-4300-9c6b-f991bbcc5c7b",
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "I am not sure. Why can't the command just fail, and we put the retry responsibility on the client? I tend to shy away from ad-hoc retry logics, because you have to make arbitrary decisions which you might not be entitled to. In this case the commands in-flight have time related constraints for instance, so how long should we retry them?",
        "createdAt" : "2019-06-04T08:15:11Z",
        "updatedAt" : "2019-06-04T09:19:19Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "47998027-3bf7-4569-888e-d213fa6120f4",
        "parentId" : "ef90bb11-8272-4300-9c6b-f991bbcc5c7b",
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "Another thing is that a Postgres database can be down for a long time. A robust application should survive that and recover when it comes back. ",
        "createdAt" : "2019-06-04T08:16:23Z",
        "updatedAt" : "2019-06-04T09:19:19Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "43fbfa98-b844-4be5-90f0-2ab8f1542307",
        "parentId" : "ef90bb11-8272-4300-9c6b-f991bbcc5c7b",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "Is the command/transaction actually rejected? This would then trigger another write to the database that would fail, right?",
        "createdAt" : "2019-06-04T09:09:30Z",
        "updatedAt" : "2019-06-04T09:19:19Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "0e562e5a-02c6-4321-a5e7-88491069db2f",
        "parentId" : "ef90bb11-8272-4300-9c6b-f991bbcc5c7b",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "Discussed offline: the postgres transaction will fail and thus be rolled back by `HikariJdbcConnectionProvider`. Since the client will never get a completion or a transaction, it should run into a command timeout and retry the command.",
        "createdAt" : "2019-06-04T09:31:48Z",
        "updatedAt" : "2019-06-04T09:31:49Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "13bdebb80c5a71ce211dbac7c888c01f2468a665",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +186,190 @@                    //recovering from the failure so the persistence stream doesn't die\n                    logger.error(s\"Failed to persist entry with offset: $offset\", t)\n                    ()\n                }\n          })"
  },
  {
    "id" : "af14259a-a353-49ae-9c52-383614e7b12c",
    "prId" : 990,
    "comments" : [
      {
        "id" : "b85df9b2-01ed-44f8-9605-46a2e9f25744",
        "parentId" : null,
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "it's not your fault, but I just noticed that we have a race in the persistence pipeline. As the entries can be persisted concurrently, they might get saved with non strictly increasing record times. The question is: When `offsetT2 > offsetT1 ` is/must `recordTimeT2 >= recordTimeT1` implied from it?",
        "createdAt" : "2019-05-08T09:21:48Z",
        "updatedAt" : "2019-05-09T08:23:50Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "2d72e217-9841-4a98-b0b6-aff1d1ecc1de",
        "parentId" : "b85df9b2-01ed-44f8-9605-46a2e9f25744",
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "I take it back. I forgot that it's actually called via a callback, where the order is guaranteed. ",
        "createdAt" : "2019-05-08T10:58:23Z",
        "updatedAt" : "2019-05-09T08:23:50Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "09bbdf689d930ffefebf0ef91ebc9a3d81880874",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +257,261 @@          tx.workflowId,\n          tx.ledgerEffectiveTime,\n          recordTime,\n          mappedTx,\n          mappedDisclosure"
  },
  {
    "id" : "dca2b734-8766-4adb-a9e4-ea2bf55c912b",
    "prId" : 959,
    "comments" : [
      {
        "id" : "69520144-13af-47e3-84ed-52dbf67b890a",
        "parentId" : null,
        "author" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "body" : "@francesco-da This part was written while resolving a rebase conflict. It assigns an absolute ledger offset to each ledger entry produced by the scenario loader, and gives that whole list to the DAO to persist (along with the list of active contracts produced by the scenario loader).\r\n\r\nWhile writing this, I realize that because we are using the contracts produced by the scenario loader, the SQL backend will not persist contracts that have been created *and* archived within a scenario, so it will not be able to produce a valid ledger snapshot before the end of the scenario. I don't think this is an issue, just worth mentioning.",
        "createdAt" : "2019-05-15T14:13:22Z",
        "updatedAt" : "2019-05-15T16:27:45Z",
        "lastEditedBy" : {
          "login" : "rautenrieth-da",
          "name" : "Robert Autenrieth",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/31539813?u=e5fe17e2c6f986e9ee04c5b9ca5f6a5a90d1c94a&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "e4dc5b75fb95575adb8e21bb2ae04bcccdc97acf",
    "line" : 249,
    "diffHunk" : "@@ -1,1 +394,398 @@                  entriesWithOffset._2,\n                  entriesWithOffset._1)\n              } yield { initialId }\n\n          }(DEC)"
  },
  {
    "id" : "efea578a-a083-4d51-bb54-6716ca6ecce4",
    "prId" : 752,
    "comments" : [
      {
        "id" : "bc192565-be82-43cb-b70c-c7d640db9edd",
        "parentId" : null,
        "author" : {
          "login" : "bitonic",
          "name" : "Francesco Mazzoli",
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/556090?v=4"
        },
        "body" : "```suggestion\r\n    // We process the requests in batches when under pressure (see semantics of `batch`). Note\r\n    // that this is safe on the read end because the readers rely on the dispatchers to know the\r\n    // ledger end, and not the database itself. This means that they will not start reading from the new\r\n    // ledger end until we tell them so, which we do when _all_ the entries have been committed.\r\n    mergedSources\r\n```",
        "createdAt" : "2019-04-29T16:10:26Z",
        "updatedAt" : "2019-04-29T16:12:32Z",
        "lastEditedBy" : {
          "login" : "bitonic",
          "name" : "Francesco Mazzoli",
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/556090?v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "274ca7de6c735c916257d57e61908ae8f5777ca0",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +139,143 @@    // ledger end, and not the database itself. This means that they will not start reading from the new\n    // ledger end until we tell them so, which we do when _all_ the entries have been committed.\n    mergedSources\n      .batch(noOfShortLivedConnections * 2L, e => Queue(e))((batch, e) => batch :+ e)\n      .mapAsync(1) { queue =>"
  },
  {
    "id" : "30179c2d-eb5a-4b89-bab2-7d3efd2f9a97",
    "prId" : 611,
    "comments" : [
      {
        "id" : "1763af22-7398-491a-9d0f-bb80140de2fe",
        "parentId" : null,
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "doesn't make a difference in performance according to my local tests",
        "createdAt" : "2019-04-23T07:17:16Z",
        "updatedAt" : "2019-04-24T10:54:57Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "80a155bf0253163f8e06e1fb42f82f22e3ae95fe",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +63,67 @@    implicit val ec: ExecutionContext = DirectExecutionContext\n\n    val noOfShortLivedConnections = 8\n    val noOfStreamingConnections = 4\n"
  }
]