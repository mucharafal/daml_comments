[
  {
    "id" : "456a6e65-b489-485e-a51d-34ed01ffb189",
    "prId" : 2022,
    "comments" : [
      {
        "id" : "8890f969-f14a-4d8c-b792-2b1cdfb65ecc",
        "parentId" : null,
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "I don't think this needs to be logged at warn. INFO at most, but I'd also be happy with DEBUG.",
        "createdAt" : "2019-07-05T14:05:03Z",
        "updatedAt" : "2019-07-05T14:09:44Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "d2755752-5eb2-428d-8ab8-d8704f1b94b7",
        "parentId" : "8890f969-f14a-4d8c-b792-2b1cdfb65ecc",
        "author" : {
          "login" : "stefanobaghino-da",
          "name" : "Stefano Baghino",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/43749967?v=4"
        },
        "body" : "As discussed offline, since we agreed to switch from `List`s to individual packages, this piece of code will likely be removed in the following days, so I'll leave it as is.",
        "createdAt" : "2019-07-05T18:27:24Z",
        "updatedAt" : "2019-07-05T18:27:25Z",
        "lastEditedBy" : {
          "login" : "stefanobaghino-da",
          "name" : "Stefano Baghino",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/43749967?v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "8aca858d87707729f879490f8b4a558ecb8ec2af",
    "line" : 29,
    "diffHunk" : "@@ -170,9 +173,25 @@ class PostgresIndexer private (\n       case PartyAddedToParticipant(party, displayName, _, _) =>\n         ledgerDao.storeParty(party, Some(displayName)).map(_ => ())(DEC)\n \n-      case PublicPackagesUploaded(_, _, _, _) =>\n-        // TODO (GS) implement once https://github.com/digital-asset/daml/pull/1610 lands\n-        Future.successful(())\n+      case PublicPackagesUploaded(archives, _, participantId, _) if archives.isEmpty =>\n+        Future.successful {\n+          PostgresIndexer.logger.warn("
  },
  {
    "id" : "cf891399-875a-46c3-8bb4-26bd7f610cff",
    "prId" : 1978,
    "comments" : [
      {
        "id" : "113cd381-e31d-4323-a26a-089d90d2c479",
        "parentId" : null,
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "I wonder if using `require` would be more idiomatic than explicitly crafting Exceptions.",
        "createdAt" : "2019-07-03T07:32:12Z",
        "updatedAt" : "2019-07-03T12:30:17Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
        ]
      },
      {
        "id" : "0f3470b1-1b37-41a7-b3f0-4ef07055edcb",
        "parentId" : "113cd381-e31d-4323-a26a-089d90d2c479",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "Considering that I want to do some logging in either case, I'd rather not repeat the condition for the `require`. I think \"crafting\" the right exception by hand is quite appropriate here.",
        "createdAt" : "2019-07-03T11:15:54Z",
        "updatedAt" : "2019-07-03T12:30:17Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "489014fcd84217503412500523ac71e792b0c3aa",
    "line" : 109,
    "diffHunk" : "@@ -0,0 +1,263 @@\n+// Copyright (c) 2019 Digital Asset (Switzerland) GmbH and/or its affiliates. All rights reserved.\n+// SPDX-License-Identifier: Apache-2.0\n+\n+package com.digitalasset.platform.index\n+\n+import java.time.Instant\n+\n+import akka.actor.ActorSystem\n+import akka.stream._\n+import akka.stream.scaladsl.{Keep, Sink}\n+import akka.{Done, NotUsed}\n+import com.daml.ledger.participant.state.v2.Update._\n+import com.daml.ledger.participant.state.v2._\n+import com.digitalasset.daml.lf.data.Ref.LedgerString\n+import com.digitalasset.daml.lf.engine.Blinding\n+import com.digitalasset.daml.lf.value.Value.{AbsoluteContractId, ContractId}\n+import com.digitalasset.ledger.api.domain\n+import com.digitalasset.ledger.api.domain.LedgerId\n+import com.digitalasset.platform.common.util.{DirectExecutionContext => DEC}\n+import com.digitalasset.platform.sandbox.metrics.MetricsManager\n+import com.digitalasset.platform.sandbox.services.transaction.SandboxEventIdFormatter\n+import com.digitalasset.platform.sandbox.stores.ledger.LedgerEntry\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.SqlLedger.{\n+  noOfShortLivedConnections,\n+  noOfStreamingConnections\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.dao.{\n+  LedgerDao,\n+  PersistenceEntry,\n+  PostgresLedgerDao\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.serialisation.{\n+  ContractSerializer,\n+  KeyHasher,\n+  TransactionSerializer,\n+  ValueSerializer\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.util.DbDispatcher\n+import org.slf4j.LoggerFactory\n+import scalaz.syntax.tag._\n+\n+import scala.concurrent.duration._\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.util.control.NonFatal\n+import scala.util.{Failure, Success}\n+\n+object PostgresIndexer {\n+  private val logger = LoggerFactory.getLogger(classOf[PostgresIndexer])\n+  private[index] val asyncTolerance = 30.seconds\n+\n+  def create(readService: ReadService, jdbcUrl: String): Future[PostgresIndexer] = {\n+    val actorSystem = ActorSystem(\"postgres-indexer\")\n+    val materializer: ActorMaterializer = ActorMaterializer()(actorSystem)\n+    val metricsManager = MetricsManager(false)\n+\n+    implicit val ec: ExecutionContext = DEC\n+\n+    val ledgerInit = readService\n+      .getLedgerInitialConditions()\n+      .runWith(Sink.head)(materializer)\n+\n+    val ledgerDao: LedgerDao = initializeDao(jdbcUrl, metricsManager)\n+    for {\n+      LedgerInitialConditions(ledgerIdString, _, _) <- ledgerInit\n+      ledgerId = domain.LedgerId(ledgerIdString)\n+      _ <- initializeLedger(ledgerId, ledgerDao)\n+      ledgerEnd <- ledgerDao.lookupLedgerEnd()\n+      externalOffset <- ledgerDao.lookupExternalLedgerEnd()\n+    } yield {\n+      new PostgresIndexer(ledgerEnd, externalOffset, ledgerDao)(materializer) {\n+        override def close(): Unit = {\n+          super.close()\n+          materializer.shutdown()\n+          Await.result(actorSystem.terminate(), asyncTolerance)\n+          metricsManager.close()\n+        }\n+      }\n+    }\n+  }\n+\n+  private def initializeDao(jdbcUrl: String, mm: MetricsManager) = {\n+    val dbDispatcher = DbDispatcher(jdbcUrl, noOfShortLivedConnections, noOfStreamingConnections)\n+    val ledgerDao = LedgerDao.metered(\n+      PostgresLedgerDao(\n+        dbDispatcher,\n+        ContractSerializer,\n+        TransactionSerializer,\n+        ValueSerializer,\n+        KeyHasher))(mm)\n+    ledgerDao\n+  }\n+\n+  private def ledgerFound(foundLedgerId: LedgerId) = {\n+    logger.info(s\"Found existing ledger with id: ${foundLedgerId.unwrap}\")\n+    Future.successful(foundLedgerId)\n+  }\n+\n+  private def initializeLedger(ledgerId: domain.LedgerId, ledgerDao: LedgerDao) = {\n+    ledgerDao\n+      .lookupLedgerId()\n+      .flatMap {\n+        case Some(foundLedgerId) if foundLedgerId == ledgerId =>\n+          ledgerFound(foundLedgerId)\n+\n+        case Some(foundLedgerId) =>\n+          val errorMsg =\n+            s\"Ledger id mismatch. Ledger id given ('$ledgerId') is not equal to the existing one ('$foundLedgerId')!\"\n+          logger.error(errorMsg)\n+          Future.failed(new IllegalArgumentException(errorMsg))"
  },
  {
    "id" : "3d2bc7ed-fa9a-4054-8bbd-6c005041b471",
    "prId" : 1978,
    "comments" : [
      {
        "id" : "f14e627a-3d5b-409d-b602-7be4e7721ed5",
        "parentId" : null,
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "`beginAfter` has _exclusive_ semantics. I'd consider changing it to inclusive so it's consistent with the rest of the platform.",
        "createdAt" : "2019-07-03T07:37:43Z",
        "updatedAt" : "2019-07-03T12:30:17Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "f27cd9b6-4294-4e38-9b7a-a37e0981e244",
        "parentId" : "f14e627a-3d5b-409d-b602-7be4e7721ed5",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "We cannot do that. We get the offset from the read service (it's an opaque `Array[Long]`) and we simply store it. There is no way we can increment it, so the only thing we do with the offset from the backend is to store it and provide it to the read service when resuming the subscription to the state updates.",
        "createdAt" : "2019-07-03T11:17:25Z",
        "updatedAt" : "2019-07-03T12:30:17Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "489014fcd84217503412500523ac71e792b0c3aa",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,263 @@\n+// Copyright (c) 2019 Digital Asset (Switzerland) GmbH and/or its affiliates. All rights reserved.\n+// SPDX-License-Identifier: Apache-2.0\n+\n+package com.digitalasset.platform.index\n+\n+import java.time.Instant\n+\n+import akka.actor.ActorSystem\n+import akka.stream._\n+import akka.stream.scaladsl.{Keep, Sink}\n+import akka.{Done, NotUsed}\n+import com.daml.ledger.participant.state.v2.Update._\n+import com.daml.ledger.participant.state.v2._\n+import com.digitalasset.daml.lf.data.Ref.LedgerString\n+import com.digitalasset.daml.lf.engine.Blinding\n+import com.digitalasset.daml.lf.value.Value.{AbsoluteContractId, ContractId}\n+import com.digitalasset.ledger.api.domain\n+import com.digitalasset.ledger.api.domain.LedgerId\n+import com.digitalasset.platform.common.util.{DirectExecutionContext => DEC}\n+import com.digitalasset.platform.sandbox.metrics.MetricsManager\n+import com.digitalasset.platform.sandbox.services.transaction.SandboxEventIdFormatter\n+import com.digitalasset.platform.sandbox.stores.ledger.LedgerEntry\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.SqlLedger.{\n+  noOfShortLivedConnections,\n+  noOfStreamingConnections\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.dao.{\n+  LedgerDao,\n+  PersistenceEntry,\n+  PostgresLedgerDao\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.serialisation.{\n+  ContractSerializer,\n+  KeyHasher,\n+  TransactionSerializer,\n+  ValueSerializer\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.util.DbDispatcher\n+import org.slf4j.LoggerFactory\n+import scalaz.syntax.tag._\n+\n+import scala.concurrent.duration._\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.util.control.NonFatal\n+import scala.util.{Failure, Success}\n+\n+object PostgresIndexer {\n+  private val logger = LoggerFactory.getLogger(classOf[PostgresIndexer])\n+  private[index] val asyncTolerance = 30.seconds\n+\n+  def create(readService: ReadService, jdbcUrl: String): Future[PostgresIndexer] = {\n+    val actorSystem = ActorSystem(\"postgres-indexer\")\n+    val materializer: ActorMaterializer = ActorMaterializer()(actorSystem)\n+    val metricsManager = MetricsManager(false)\n+\n+    implicit val ec: ExecutionContext = DEC\n+\n+    val ledgerInit = readService\n+      .getLedgerInitialConditions()\n+      .runWith(Sink.head)(materializer)\n+\n+    val ledgerDao: LedgerDao = initializeDao(jdbcUrl, metricsManager)\n+    for {\n+      LedgerInitialConditions(ledgerIdString, _, _) <- ledgerInit\n+      ledgerId = domain.LedgerId(ledgerIdString)\n+      _ <- initializeLedger(ledgerId, ledgerDao)\n+      ledgerEnd <- ledgerDao.lookupLedgerEnd()\n+      externalOffset <- ledgerDao.lookupExternalLedgerEnd()\n+    } yield {\n+      new PostgresIndexer(ledgerEnd, externalOffset, ledgerDao)(materializer) {\n+        override def close(): Unit = {\n+          super.close()\n+          materializer.shutdown()\n+          Await.result(actorSystem.terminate(), asyncTolerance)\n+          metricsManager.close()\n+        }\n+      }\n+    }\n+  }\n+\n+  private def initializeDao(jdbcUrl: String, mm: MetricsManager) = {\n+    val dbDispatcher = DbDispatcher(jdbcUrl, noOfShortLivedConnections, noOfStreamingConnections)\n+    val ledgerDao = LedgerDao.metered(\n+      PostgresLedgerDao(\n+        dbDispatcher,\n+        ContractSerializer,\n+        TransactionSerializer,\n+        ValueSerializer,\n+        KeyHasher))(mm)\n+    ledgerDao\n+  }\n+\n+  private def ledgerFound(foundLedgerId: LedgerId) = {\n+    logger.info(s\"Found existing ledger with id: ${foundLedgerId.unwrap}\")\n+    Future.successful(foundLedgerId)\n+  }\n+\n+  private def initializeLedger(ledgerId: domain.LedgerId, ledgerDao: LedgerDao) = {\n+    ledgerDao\n+      .lookupLedgerId()\n+      .flatMap {\n+        case Some(foundLedgerId) if foundLedgerId == ledgerId =>\n+          ledgerFound(foundLedgerId)\n+\n+        case Some(foundLedgerId) =>\n+          val errorMsg =\n+            s\"Ledger id mismatch. Ledger id given ('$ledgerId') is not equal to the existing one ('$foundLedgerId')!\"\n+          logger.error(errorMsg)\n+          Future.failed(new IllegalArgumentException(errorMsg))\n+\n+        case None =>\n+          logger.info(s\"Initializing ledger with id: ${ledgerId.unwrap}\")\n+          ledgerDao.initializeLedger(ledgerId, 0).map(_ => ledgerId)(DEC)\n+      }(DEC)\n+  }\n+}\n+\n+class PostgresIndexer(initialOffset: Long, beginAfter: Option[LedgerString], ledgerDao: LedgerDao)("
  },
  {
    "id" : "6d8a4492-77ef-47ea-8938-13bcbf5cff3b",
    "prId" : 1978,
    "comments" : [
      {
        "id" : "48887d63-878b-40d6-aced-f9d5a92ee1a3",
        "parentId" : null,
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "Since you never read this outside of the stream, I'd rather use an iterator zipped into the stream below to generate the offsets. That way you can eliminate the `var`.",
        "createdAt" : "2019-07-03T07:52:17Z",
        "updatedAt" : "2019-07-03T12:30:17Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "44847d39-b444-4aa8-a709-e9ddd1957b72",
        "parentId" : "48887d63-878b-40d6-aced-f9d5a92ee1a3",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "That's not quite true, because we don't increment the offset for all events we receive from the read service, so we would be producing noncontiguous offsets which would be fine I guess?",
        "createdAt" : "2019-07-03T11:32:28Z",
        "updatedAt" : "2019-07-03T12:30:17Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "7d409960-9acf-43ce-8bab-9a2ee5012dee",
        "parentId" : "48887d63-878b-40d6-aced-f9d5a92ee1a3",
        "author" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "body" : "it depends on when you are merging the offset generator in. If you do it right before you need to use them you're fine.",
        "createdAt" : "2019-07-03T13:22:24Z",
        "updatedAt" : "2019-07-03T13:22:24Z",
        "lastEditedBy" : {
          "login" : "gabor-aranyossy",
          "name" : "Gabor Aranyossy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/29858875?u=03b5eca13ed32e74056ee216f2def12e249bcfed&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "489014fcd84217503412500523ac71e792b0c3aa",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,263 @@\n+// Copyright (c) 2019 Digital Asset (Switzerland) GmbH and/or its affiliates. All rights reserved.\n+// SPDX-License-Identifier: Apache-2.0\n+\n+package com.digitalasset.platform.index\n+\n+import java.time.Instant\n+\n+import akka.actor.ActorSystem\n+import akka.stream._\n+import akka.stream.scaladsl.{Keep, Sink}\n+import akka.{Done, NotUsed}\n+import com.daml.ledger.participant.state.v2.Update._\n+import com.daml.ledger.participant.state.v2._\n+import com.digitalasset.daml.lf.data.Ref.LedgerString\n+import com.digitalasset.daml.lf.engine.Blinding\n+import com.digitalasset.daml.lf.value.Value.{AbsoluteContractId, ContractId}\n+import com.digitalasset.ledger.api.domain\n+import com.digitalasset.ledger.api.domain.LedgerId\n+import com.digitalasset.platform.common.util.{DirectExecutionContext => DEC}\n+import com.digitalasset.platform.sandbox.metrics.MetricsManager\n+import com.digitalasset.platform.sandbox.services.transaction.SandboxEventIdFormatter\n+import com.digitalasset.platform.sandbox.stores.ledger.LedgerEntry\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.SqlLedger.{\n+  noOfShortLivedConnections,\n+  noOfStreamingConnections\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.dao.{\n+  LedgerDao,\n+  PersistenceEntry,\n+  PostgresLedgerDao\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.serialisation.{\n+  ContractSerializer,\n+  KeyHasher,\n+  TransactionSerializer,\n+  ValueSerializer\n+}\n+import com.digitalasset.platform.sandbox.stores.ledger.sql.util.DbDispatcher\n+import org.slf4j.LoggerFactory\n+import scalaz.syntax.tag._\n+\n+import scala.concurrent.duration._\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.util.control.NonFatal\n+import scala.util.{Failure, Success}\n+\n+object PostgresIndexer {\n+  private val logger = LoggerFactory.getLogger(classOf[PostgresIndexer])\n+  private[index] val asyncTolerance = 30.seconds\n+\n+  def create(readService: ReadService, jdbcUrl: String): Future[PostgresIndexer] = {\n+    val actorSystem = ActorSystem(\"postgres-indexer\")\n+    val materializer: ActorMaterializer = ActorMaterializer()(actorSystem)\n+    val metricsManager = MetricsManager(false)\n+\n+    implicit val ec: ExecutionContext = DEC\n+\n+    val ledgerInit = readService\n+      .getLedgerInitialConditions()\n+      .runWith(Sink.head)(materializer)\n+\n+    val ledgerDao: LedgerDao = initializeDao(jdbcUrl, metricsManager)\n+    for {\n+      LedgerInitialConditions(ledgerIdString, _, _) <- ledgerInit\n+      ledgerId = domain.LedgerId(ledgerIdString)\n+      _ <- initializeLedger(ledgerId, ledgerDao)\n+      ledgerEnd <- ledgerDao.lookupLedgerEnd()\n+      externalOffset <- ledgerDao.lookupExternalLedgerEnd()\n+    } yield {\n+      new PostgresIndexer(ledgerEnd, externalOffset, ledgerDao)(materializer) {\n+        override def close(): Unit = {\n+          super.close()\n+          materializer.shutdown()\n+          Await.result(actorSystem.terminate(), asyncTolerance)\n+          metricsManager.close()\n+        }\n+      }\n+    }\n+  }\n+\n+  private def initializeDao(jdbcUrl: String, mm: MetricsManager) = {\n+    val dbDispatcher = DbDispatcher(jdbcUrl, noOfShortLivedConnections, noOfStreamingConnections)\n+    val ledgerDao = LedgerDao.metered(\n+      PostgresLedgerDao(\n+        dbDispatcher,\n+        ContractSerializer,\n+        TransactionSerializer,\n+        ValueSerializer,\n+        KeyHasher))(mm)\n+    ledgerDao\n+  }\n+\n+  private def ledgerFound(foundLedgerId: LedgerId) = {\n+    logger.info(s\"Found existing ledger with id: ${foundLedgerId.unwrap}\")\n+    Future.successful(foundLedgerId)\n+  }\n+\n+  private def initializeLedger(ledgerId: domain.LedgerId, ledgerDao: LedgerDao) = {\n+    ledgerDao\n+      .lookupLedgerId()\n+      .flatMap {\n+        case Some(foundLedgerId) if foundLedgerId == ledgerId =>\n+          ledgerFound(foundLedgerId)\n+\n+        case Some(foundLedgerId) =>\n+          val errorMsg =\n+            s\"Ledger id mismatch. Ledger id given ('$ledgerId') is not equal to the existing one ('$foundLedgerId')!\"\n+          logger.error(errorMsg)\n+          Future.failed(new IllegalArgumentException(errorMsg))\n+\n+        case None =>\n+          logger.info(s\"Initializing ledger with id: ${ledgerId.unwrap}\")\n+          ledgerDao.initializeLedger(ledgerId, 0).map(_ => ledgerId)(DEC)\n+      }(DEC)\n+  }\n+}\n+\n+class PostgresIndexer(initialOffset: Long, beginAfter: Option[LedgerString], ledgerDao: LedgerDao)(\n+    implicit mat: Materializer)\n+    extends Indexer\n+    with AutoCloseable {\n+\n+  @volatile\n+  private var headRef = initialOffset"
  }
]