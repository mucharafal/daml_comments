[
  {
    "id" : "475971a0-52fd-4c4e-afbc-3bd1843ba3ae",
    "prId" : 7460,
    "comments" : [
      {
        "id" : "b9a685ee-905b-43c6-9937-cdc0f4d5efc9",
        "parentId" : null,
        "author" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "body" : "Do we need synchronous preloading ? ",
        "createdAt" : "2020-09-29T14:18:43Z",
        "updatedAt" : "2020-10-08T09:30:43Z",
        "lastEditedBy" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "f0104407-59e9-4958-9967-e390d0cef3e3",
        "parentId" : "b9a685ee-905b-43c6-9937-cdc0f4d5efc9",
        "author" : {
          "login" : "dajmaki",
          "name" : "Jussi M채ki",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/19684330?u=abb996187e6b472110abdd8dc3c27c4fc0140b92&v=4"
        },
        "body" : "No I don't think we need that, unless it's more efficient to validate+preload when committing rather than validating and loading the package to engine separately.",
        "createdAt" : "2020-09-29T14:40:28Z",
        "updatedAt" : "2020-10-08T09:30:43Z",
        "lastEditedBy" : {
          "login" : "dajmaki",
          "name" : "Jussi M채ki",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/19684330?u=abb996187e6b472110abdd8dc3c27c4fc0140b92&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "7c488ea3-672d-49a2-ae83-cbf18c4bd708",
        "parentId" : "b9a685ee-905b-43c6-9937-cdc0f4d5efc9",
        "author" : {
          "login" : "dajmaki",
          "name" : "Jussi M채ki",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/19684330?u=abb996187e6b472110abdd8dc3c27c4fc0140b92&v=4"
        },
        "body" : "Actually, I think the system is more predictable if we preload it synchronously when committing as that means as soon as the package upload completes we can start using the package without having a surprising delay on the first transaction. What do you think?",
        "createdAt" : "2020-09-29T14:41:19Z",
        "updatedAt" : "2020-10-08T09:30:43Z",
        "lastEditedBy" : {
          "login" : "dajmaki",
          "name" : "Jussi M채ki",
          "avatarUrl" : "https://avatars0.githubusercontent.com/u/19684330?u=abb996187e6b472110abdd8dc3c27c4fc0140b92&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "13eed54d-708a-4ff9-9f17-2f41afb1819d",
        "parentId" : "b9a685ee-905b-43c6-9937-cdc0f4d5efc9",
        "author" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "body" : "I see a trade off: \r\n* If we preload we are indeed more predictable. It is also slightly more efficient as we decode only once. (I think our bigger dars need around 20s to decode).\r\n* If we don't, we do not overload the engine with useless packages. Potentially the code for scenario/trigger/script can end in the dar even it is useless for the Ledger engine. ",
        "createdAt" : "2020-09-29T15:22:50Z",
        "updatedAt" : "2020-10-08T09:30:43Z",
        "lastEditedBy" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "ff8ed3e3-8051-4919-8bc1-46f4d171de8e",
        "parentId" : "b9a685ee-905b-43c6-9937-cdc0f4d5efc9",
        "author" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "body" : "I see the feature useful for testing/development. In production, dar upload should be so seldom that it should not really matter. ",
        "createdAt" : "2020-09-29T15:25:37Z",
        "updatedAt" : "2020-10-08T09:30:43Z",
        "lastEditedBy" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "7004d0dd-febf-4660-9023-36110952943f",
        "parentId" : "b9a685ee-905b-43c6-9937-cdc0f4d5efc9",
        "author" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "body" : "I think the preload should by synchronous for the pro-preload reasons you both mentioned.",
        "createdAt" : "2020-09-30T15:40:59Z",
        "updatedAt" : "2020-10-08T09:30:43Z",
        "lastEditedBy" : {
          "login" : "gerolf-da",
          "name" : "Gerolf Seitz",
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/29121423?u=f683aa614e742c653ae8c01b194905dcdef6e974&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "8652e429-09d7-4135-91f8-96bf69bc6a6f",
        "parentId" : "b9a685ee-905b-43c6-9937-cdc0f4d5efc9",
        "author" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "body" : "We will preload in the 3 cases.",
        "createdAt" : "2020-10-02T11:49:44Z",
        "updatedAt" : "2020-10-08T09:30:43Z",
        "lastEditedBy" : {
          "login" : "remyhaemmerle-da",
          "name" : "Remy",
          "avatarUrl" : "https://avatars2.githubusercontent.com/u/45566104?u=10d0caf8150a686217852f90e91314a4de3239a9&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "0a4599bcbf38cacac4088b9b73021b01c997c440",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,154 @@\n+package com.daml.ledger.participant.state.kvutils\n+package committer\n+\n+import com.daml.lf.archive\n+import com.daml.lf.data.Ref\n+import com.daml.lf.engine.Engine\n+import com.daml.lf.language.Ast\n+import com.daml.metrics.Metrics\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n+\n+/*\n+ * This Package Committer performs a full package validation before committing.\n+ * The kvutils integrations should handle long-running submissions (> 10s).\n+ *\n+ * When using this committer, packages on the ledger can be trusted and do not\n+ * have to be revalidated when loading in the engine.\n+ */\n+private[committer] class PackageValidator(\n+    engine: Engine,\n+    metrics: Metrics,\n+) extends PackageCommitter[Map[Ref.PackageId, Ast.Package]](metrics) {\n+\n+  import Committer._\n+\n+  override protected val committerName = \"package_validator\"\n+\n+  metrics.daml.kvutils.committer.packageUpload.loadedPackages(() =>\n+    engine.compiledPackages().packageIds.size)\n+\n+  def addFailure[X](either: Either[List[String], X], msg: String): Either[List[String], X] =\n+    Left(msg :: either.left.getOrElse(Nil))\n+\n+  private[this] val decodeEntry: Step = {\n+    case (ctx, (uploadEntry, None)) =>\n+      type Result = Either[List[String], Map[Ref.PackageId, Ast.Package]]\n+\n+      val result: Result =\n+        metrics.daml.kvutils.committer.packageUpload.decodeTimer.time { () =>\n+          // toSet is constant time here.\n+          val knownPackages = engine.compiledPackages().packageIds.toSet[String]\n+\n+          uploadEntry.getArchivesList.iterator().asScala.foldLeft[Result](Right(Map.empty)) {\n+            (acc, arch) =>\n+              try {\n+                if (knownPackages(arch.getHash)) {\n+                  // If the package is already known, we don't decode it but verify its Hash.\n+                  archive.Reader.HashChecker.decodeArchive(arch)\n+                  acc\n+                } else {\n+                  acc.map(_ + archive.Decode.decodeArchive(arch))\n+                }\n+              } catch {\n+                case NonFatal(e) =>\n+                  Left(\n+                    s\"Cannot decode archive ${arch.getHash}: ${e.getMessage}\" :: acc.left\n+                      .getOrElse(Nil))\n+              }\n+          }\n+        }\n+\n+      result match {\n+        case Right(newPkgs) =>\n+          StepContinue((uploadEntry, Some(newPkgs)))\n+        case Left(errors) =>\n+          val msg = errors.mkString(\", \")\n+          rejectionTraceLog(msg, uploadEntry)\n+          reject(\n+            ctx.getRecordTime,\n+            uploadEntry.getSubmissionId,\n+            uploadEntry.getParticipantId,\n+            _.setInvalidPackage(DamlKvutils.Invalid.newBuilder.setDetails(msg))\n+          )\n+      }\n+  }\n+\n+  // validate package that have been decoded in a previous step.\n+  private[this] val validateEntry: Step = {\n+    case (ctx, partialResult @ (uploadEntry, Some(pkgs))) =>\n+      type Result = Either[List[String], Unit]\n+\n+      val result: Result = metrics.daml.kvutils.committer.packageUpload.validateTimer.time { () =>\n+        val allPkgIds = uploadEntry.getArchivesList\n+          .iterator()\n+          .asScala\n+          .map(pkg => Ref.PackageId.assertFromString(pkg.getHash))\n+          .toSet\n+\n+        pkgs.keys.foldLeft[Result](Right(()))(\n+          (acc, pkgId) =>\n+            engine\n+              .validatePackages(allPkgIds, pkgs)\n+              .fold(\n+                err => Left(s\"Cannot decode archive ${pkgId}: ${err}\" :: acc.left.getOrElse(Nil)),\n+                _ => acc\n+            )\n+        )\n+      }\n+\n+      result match {\n+        case Right(_) =>\n+          StepContinue(partialResult)\n+        case Left(errors) =>\n+          val msg = errors.mkString(\", \")\n+          rejectionTraceLog(msg, uploadEntry)\n+          reject(\n+            ctx.getRecordTime,\n+            uploadEntry.getSubmissionId,\n+            uploadEntry.getParticipantId,\n+            _.setInvalidPackage(DamlKvutils.Invalid.newBuilder.setDetails(msg))\n+          )\n+      }\n+  }\n+\n+  private val preload: Step = {\n+    case (ctx, partialResult @ (uploadEntry, Some(pkgs))) =>\n+      val errors: List[String] =\n+        metrics.daml.kvutils.committer.packageUpload.preloadTimer.time { () =>\n+          pkgs.iterator.flatMap {\n+            case (pkgId, pkg) =>\n+              engine\n+                .preloadPackage(pkgId, pkg)\n+                .consume(_ => None, pkgs.get, _ => None)\n+                .fold(err => List(err.detailMsg), _ => List.empty)\n+          }.toList\n+        }\n+\n+      if (errors.isEmpty)\n+        StepContinue(partialResult)\n+      else {\n+        val msg = errors.mkString(\", \")\n+        rejectionTraceLog(msg, uploadEntry)\n+        reject(\n+          ctx.getRecordTime,\n+          uploadEntry.getSubmissionId,\n+          uploadEntry.getParticipantId,\n+          _.setInvalidPackage(DamlKvutils.Invalid.newBuilder.setDetails(msg))\n+        )\n+      }\n+  }\n+\n+  override protected val steps: Iterable[(StepInfo, Step)] =\n+    List(\n+      \"authorize_submission\" -> authorizeSubmission,\n+      \"deduplicate_submission\" -> deduplicateSubmission,\n+      \"decode_entry\" -> decodeEntry,\n+      \"validate_entry\" -> validateEntry,\n+      \"filter_duplicates\" -> filterDuplicates,\n+      \"preload\" -> preload,"
  }
]